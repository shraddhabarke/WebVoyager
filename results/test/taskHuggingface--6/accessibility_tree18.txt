[1781] RootWebArea 'sentence-transformers/all-MiniLM-L6-v2 ¬∑ Hugging Face' focused: True url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
	[2096] main ''
		[2616] button ''
		[2728] code ''
			[2747] StaticText 'print'
			[2748] StaticText '(embeddings)\n'
		[2755] heading 'Usage (HuggingFace Transformers)'
			[2757] link '' url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#usage-huggingface-transformers
		[2769] StaticText 'Without '
		[2770] link 'sentence-transformers' url: https://www.sbert.net/
		[2772] StaticText ', you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.'
		[2776] code ''
			[2780] StaticText 'from'
			[2781] StaticText ' transformers '
			[2783] StaticText 'import'
			[2784] StaticText ' AutoTokenizer, AutoModel\n'
			[2787] StaticText ' torch\n'
			[2790] StaticText ' torch.nn.functional '
			[2792] StaticText 'as'
			[2793] StaticText ' F\n\n'
			[2795] StaticText '#Mean Pooling - Take attention mask into account for correct averaging'
			[2798] StaticText 'def'
			[2801] StaticText 'mean_pooling'
			[2802] StaticText '('
			[2804] StaticText 'model_output, attention_mask'
			[2805] StaticText '):\n    token_embeddings = model_output['
			[2810] StaticText '#First element of model_output contains all token embeddings'
			[2811] StaticText '\n    input_mask_expanded = attention_mask.unsqueeze(-'
			[2814] StaticText ').expand(token_embeddings.size()).'
			[2816] StaticText 'float'
			[2817] StaticText '()\n    '
			[2819] StaticText 'return'
			[2820] StaticText ' torch.'
			[2822] StaticText 'sum'
			[2823] StaticText '(token_embeddings * input_mask_expanded, '
			[2825] StaticText '1'
			[2826] StaticText ') / torch.clamp(input_mask_expanded.'
			[2828] StaticText 'sum'
			[2832] StaticText '), '
			[2834] StaticText 'min'
			[2835] StaticText '='
			[2837] StaticText '1e-9'
			[2838] StaticText ')\n\n\n'
			[2840] StaticText '# Sentences we want sentence embeddings for'
			[2841] StaticText '\nsentences = ['
			[2844] StaticText ', '
			[2847] StaticText ']\n\n'
			[2849] StaticText '# Load model from HuggingFace Hub'
			[2850] StaticText '\ntokenizer = AutoTokenizer.from_pretrained('
			[2853] StaticText ')\nmodel = AutoModel.from_pretrained('
			[2856] StaticText ')\n\n'
			[2858] StaticText '# Tokenize sentences'
			[2859] StaticText '\nencoded_input = tokenizer(sentences, padding='
			[2861] StaticText 'True'
			[2862] StaticText ', truncation='
			[2865] StaticText ', return_tensors='
			[2868] StaticText ')\n\n'
			[2870] StaticText '# Compute token embeddings'
			[2873] StaticText 'with'
			[2874] StaticText ' torch.no_grad():\n    model_output = model(**encoded_input)\n\n'
			[2876] StaticText '# Perform pooling'
			[2877] StaticText '\nsentence_embeddings = mean_pooling(model_output, encoded_input['
			[2880] StaticText '])\n\n'
			[2882] StaticText '# Normalize embeddings'
			[2883] StaticText '\nsentence_embeddings = F.normalize(sentence_embeddings, p='
			[2886] StaticText ', dim='
			[2888] StaticText '1'
			[2889] StaticText ')\n\n'
			[2891] StaticText 'print'
			[2892] StaticText '('
			[2894] StaticText '"Sentence embeddings:"'
			[2895] StaticText ')\n'
			[2897] StaticText 'print'
			[2898] StaticText '(sentence_embeddings)\n'
		[2899] button ''
		[2905] separator '' orientation: horizontal
		[2907] heading 'Background'
			[2909] link '' url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#background
		[2921] StaticText 'The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective. We used the pretrained '
		[2922] link 'nreimers/MiniLM-L6-H384-uncased' url: https://huggingface.co/nreimers/MiniLM-L6-H384-uncased
			[2923] code ''
		[2927] StaticText ' model and fine-tuned in on a 1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.'
		[2930] StaticText 'We developed this model during the '
		[2931] link 'Community week using JAX/Flax for NLP & CV' url: https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104
		[2933] StaticText ', organized by Hugging Face. We developed this model as part of the project: '
		[3734] button 'Compute'
		[3746] button 'View Code'
		[3758] button 'Maximize'
		[3774] heading 'Model tree for sentence-transformers/all-MiniLM-L6-v2'
		[3795] image ''
		[3799] StaticText 'Adapters'
		[3803] link '3 models' url: https://huggingface.co/models?other=base_model:adapter:sentence-transformers/all-MiniLM-L6-v2
		[3810] image ''
		[3814] StaticText 'Finetunes'
		[3818] link '337 models' url: https://huggingface.co/models?other=base_model:finetune:sentence-transformers/all-MiniLM-L6-v2
		[3825] image ''
		[3829] StaticText 'Quantizations'
		[3833] link '27 models' url: https://huggingface.co/models?other=base_model:quantized:sentence-transformers/all-MiniLM-L6-v2
		[3839] heading 'Datasets used to train sentence-transformers/all-MiniLM-L6-v2'
		[3850] article ''
			[3851] link 'Viewer ‚Ä¢ Updated Jan 5, 2024 ‚Ä¢ 848k ‚Ä¢ 25.9k ‚Ä¢ 129' url: https://huggingface.co/datasets/mandarjoshi/trivia_qa
				[3853] sectionheader ''
					[3860] heading 'mandarjoshi/trivia_qa'
				[3873] time 'Fri, 05 Jan 2024 13:24:37 GMT'
					[3874] StaticText 'Jan 5, 2024'
				[3877] StaticText '‚Ä¢'
				[3883] StaticText '848k'
				[3890] StaticText '25.9k'
				[3896] StaticText '129'
		[3898] article ''
			[3899] link 'Viewer ‚Ä¢ Updated Jan 4, 2024 ‚Ä¢ 1.11M ‚Ä¢ 7.69k ‚Ä¢ 160' url: https://huggingface.co/datasets/microsoft/ms_marco
				[3901] sectionheader ''
					[3908] heading 'microsoft/ms_marco'
				[3921] time 'Thu, 04 Jan 2024 16:01:29 GMT'
					[3922] StaticText 'Jan 4, 2024'
				[3925] StaticText '‚Ä¢'
				[3931] StaticText '1.11M'
				[3938] StaticText '7.69k'
				[3944] StaticText '160'
		[3946] article ''
			[3947] link 'Updated Jan 18, 2024 ‚Ä¢ 6.98k ‚Ä¢ 297' url: https://huggingface.co/datasets/code-search-net/code_search_net
				[3949] sectionheader ''
					[3956] heading 'code-search-net/code_search_net'
				[3964] time 'Thu, 18 Jan 2024 09:19:12 GMT'
					[3965] StaticText 'Jan 18, 2024'
				[3968] StaticText '‚Ä¢'
				[3972] StaticText '6.98k'
				[3978] StaticText '297'
		[3983] heading 'Spaces using sentence-transformers/all-MiniLM-L6-v2 100'
		[3999] link 'ü•á mteb/leaderboard' url: https://huggingface.co/spaces/mteb/leaderboard
		[4006] link 'üåç cvachet/pdf-chatbot' url: https://huggingface.co/spaces/cvachet/pdf-chatbot
		[4013] link 'üíª marcosv/InstructIR' url: https://huggingface.co/spaces/marcosv/InstructIR
		[4020] link 'üê≥ open-webui/open-webui' url: https://huggingface.co/spaces/open-webui/open-webui
		[4027] link 'üìâ JournalistsonHF/ai-scraper' url: https://huggingface.co/spaces/JournalistsonHF/ai-scraper
		[4034] link '‚öîÔ∏è mteb/arena' url: https://huggingface.co/spaces/mteb/arena
		[4041] link 'üíΩ Illia56/Ask-AI-Youtube' url: https://huggingface.co/spaces/Illia56/Ask-AI-Youtube
		[4048] link 'üèÉ nickmuchi/semantic-search-with-retrieve-and-rerank' url: https://huggingface.co/spaces/nickmuchi/semantic-search-with-retrieve-and-rerank
		[4055] link 'üíª nickmuchi/article-text-summarizer' url: https://huggingface.co/spaces/nickmuchi/article-text-summarizer
		[4062] link 'ü•á mteb/leaderboard_legacy' url: https://huggingface.co/spaces/mteb/leaderboard_legacy
		[4069] link 'üïØÔ∏èüî° radames/Candle-BERT-Semantic-Similarity-Wasm' url: https://huggingface.co/spaces/radames/Candle-BERT-Semantic-Similarity-Wasm
		[4076] link 'üöÄ Mr-TD/RAG-PDF-QnA-ChatBot' url: https://huggingface.co/spaces/Mr-TD/RAG-PDF-QnA-ChatBot
		[4089] button '+ 88 Spaces'